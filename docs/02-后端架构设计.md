# CozyChat åç«¯æ¶æ„è®¾è®¡

## 1. æ•´ä½“æ¶æ„

### 1.1 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Layer (FastAPI)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ REST API     â”‚ â”‚ WebSocket    â”‚ â”‚ OpenAI Compatible API   â”‚â”‚
â”‚  â”‚ Endpoints    â”‚ â”‚ Handler      â”‚ â”‚ /v1/chat/completions    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Business Logic Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Personality  â”‚ â”‚ User Manager â”‚ â”‚ Session Manager         â”‚â”‚
â”‚  â”‚ System       â”‚ â”‚              â”‚ â”‚                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              Core Engine Orchestrator                       â”‚â”‚
â”‚  â”‚  (ç»Ÿä¸€è°ƒåº¦ AIå¼•æ“ã€è®°å¿†ã€å·¥å…·ã€è¯­éŸ³)                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Engine Layer (å¯æ’æ‹”)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ AI Engine    â”‚ â”‚ Memory       â”‚ â”‚ Tool System             â”‚â”‚
â”‚ â”‚ - OpenAI     â”‚ â”‚ - ChromaDB   â”‚ â”‚ - Built-in Tools        â”‚â”‚
â”‚ â”‚ - Ollama     â”‚ â”‚ - Qdrant     â”‚ â”‚ - MCP Tools             â”‚â”‚
â”‚ â”‚ - LM Studio  â”‚ â”‚ - Cache      â”‚ â”‚ - Tool Registry         â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ STT Engine   â”‚ â”‚ TTS Engine   â”‚ â”‚ RealTime Engine         â”‚â”‚
â”‚ â”‚ - OpenAI     â”‚ â”‚ - OpenAI     â”‚ â”‚ - OpenAI Realtime       â”‚â”‚
â”‚ â”‚ - Tencent    â”‚ â”‚ - Tencent    â”‚ â”‚ - Custom                â”‚â”‚
â”‚ â”‚ - Custom     â”‚ â”‚ - Custom     â”‚ â”‚                         â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Layer                                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ PostgreSQL   â”‚ â”‚ Redis Cache  â”‚ â”‚ Vector DB               â”‚â”‚
â”‚ â”‚ (ç”¨æˆ·/ä¼šè¯)   â”‚ â”‚              â”‚ â”‚ (è®°å¿†å‘é‡)               â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 ç›®å½•ç»“æ„

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPIåº”ç”¨å…¥å£
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                         # APIè·¯ç”±å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ deps.py                  # ä¾èµ–æ³¨å…¥
â”‚   â”‚   â”œâ”€â”€ v1/                      # API v1ç‰ˆæœ¬
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py              # èŠå¤©æ¥å£ï¼ˆOpenAIå…¼å®¹ï¼‰
â”‚   â”‚   â”‚   â”œâ”€â”€ audio.py             # éŸ³é¢‘æ¥å£
â”‚   â”‚   â”‚   â”œâ”€â”€ users.py             # ç”¨æˆ·ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ personalities.py     # äººæ ¼ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ tools.py             # å·¥å…·ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ memory.py            # è®°å¿†ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions.py          # ä¼šè¯ç®¡ç†
â”‚   â”‚   â”‚   â””â”€â”€ monitoring.py        # ç›‘æ§æ¥å£
â”‚   â”‚   â””â”€â”€ websocket.py             # WebSocketæ¥å£
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                        # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ personality/             # äººæ ¼ç³»ç»Ÿ
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py           # äººæ ¼ç®¡ç†å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py            # äººæ ¼æ•°æ®æ¨¡å‹
â”‚   â”‚   â”‚   â””â”€â”€ loader.py            # é…ç½®åŠ è½½å™¨
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ user/                    # ç”¨æˆ·ç³»ç»Ÿ
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py           # ç”¨æˆ·ç®¡ç†å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py              # è®¤è¯æ¨¡å—
â”‚   â”‚   â”‚   â”œâ”€â”€ profile.py           # ç”¨æˆ·ç”»åƒ
â”‚   â”‚   â”‚   â””â”€â”€ permissions.py       # æƒé™ç®¡ç†
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ session/                 # ä¼šè¯ç®¡ç†
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ orchestrator.py          # æ ¸å¿ƒç¼–æ’å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ engines/                     # å¼•æ“å±‚ï¼ˆå¯æ’æ‹”ï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ai/                      # AIå¼•æ“
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py              # AIå¼•æ“åŸºç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_engine.py     # OpenAIå®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_engine.py     # Ollamaå®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ lmstudio_engine.py   # LM Studioå®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ factory.py           # å·¥å‚æ¨¡å¼åˆ›å»ºå¼•æ“
â”‚   â”‚   â”‚   â””â”€â”€ registry.py          # å¼•æ“æ³¨å†Œä¸­å¿ƒ
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ memory/                  # è®°å¿†å¼•æ“
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py              # è®°å¿†å¼•æ“åŸºç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ chromadb_engine.py   # ChromaDBå®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ qdrant_engine.py     # Qdrantå®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py             # ç¼“å­˜å±‚
â”‚   â”‚   â”‚   â””â”€â”€ manager.py           # è®°å¿†ç®¡ç†å™¨
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ tools/                   # å·¥å…·å¼•æ“
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py              # å·¥å…·åŸºç±»
â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py          # å·¥å…·æ³¨å†Œä¸­å¿ƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py           # å·¥å…·ç®¡ç†å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ builtin/             # å†…ç½®å·¥å…·
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ calculator.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ web_search.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ time_tool.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ weather_tool.py
â”‚   â”‚   â”‚   â””â”€â”€ mcp/                 # MCPå·¥å…·
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ client.py
â”‚   â”‚   â”‚       â”œâ”€â”€ discovery.py
â”‚   â”‚   â”‚       â””â”€â”€ adapters.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ voice/                   # è¯­éŸ³å¼•æ“
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ stt/                 # STTå¼•æ“
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai_stt.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tencent_stt.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tts/                 # TTSå¼•æ“
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ openai_tts.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ tencent_tts.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ factory.py
â”‚   â”‚   â”‚   â””â”€â”€ realtime/            # RealTimeå¼•æ“
â”‚   â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”œâ”€â”€ base.py
â”‚   â”‚   â”‚       â”œâ”€â”€ openai_realtime.py
â”‚   â”‚   â”‚       â””â”€â”€ factory.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ monitoring/              # ç›‘æ§å¼•æ“
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ performance.py       # æ€§èƒ½ç›‘æ§
â”‚   â”‚       â”œâ”€â”€ metrics.py           # æŒ‡æ ‡æ”¶é›†
â”‚   â”‚       â””â”€â”€ logger.py            # æ—¥å¿—ç³»ç»Ÿ
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                      # æ•°æ®æ¨¡å‹ï¼ˆSQLAlchemyï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ user.py                  # ç”¨æˆ·æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ session.py               # ä¼šè¯æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ message.py               # æ¶ˆæ¯æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ personality.py           # äººæ ¼æ¨¡å‹ï¼ˆæ•°æ®åº“å­˜å‚¨éƒ¨åˆ†ï¼‰
â”‚   â”‚   â””â”€â”€ user_profile.py          # ç”¨æˆ·ç”»åƒæ¨¡å‹
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/                     # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat.py                  # èŠå¤©ç›¸å…³schema
â”‚   â”‚   â”œâ”€â”€ user.py                  # ç”¨æˆ·ç›¸å…³schema
â”‚   â”‚   â”œâ”€â”€ personality.py           # äººæ ¼ç›¸å…³schema
â”‚   â”‚   â”œâ”€â”€ tool.py                  # å·¥å…·ç›¸å…³schema
â”‚   â”‚   â”œâ”€â”€ memory.py                # è®°å¿†ç›¸å…³schema
â”‚   â”‚   â””â”€â”€ audio.py                 # éŸ³é¢‘ç›¸å…³schema
â”‚   â”‚
â”‚   â”œâ”€â”€ db/                          # æ•°æ®åº“ç›¸å…³
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ session.py               # æ•°æ®åº“ä¼šè¯
â”‚   â”‚   â”œâ”€â”€ base.py                  # Baseç±»
â”‚   â”‚   â””â”€â”€ init_db.py               # æ•°æ®åº“åˆå§‹åŒ–
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py                # æ—¥å¿—å·¥å…·
â”‚       â”œâ”€â”€ security.py              # å®‰å…¨å·¥å…·ï¼ˆJWTç­‰ï¼‰
â”‚       â”œâ”€â”€ cache.py                 # ç¼“å­˜å·¥å…·
â”‚       â””â”€â”€ config_loader.py         # é…ç½®åŠ è½½å·¥å…·
â”‚
â”œâ”€â”€ config/                          # é…ç½®æ–‡ä»¶ï¼ˆYAMLï¼‰
â”‚   â”œâ”€â”€ system.yaml                  # ç³»ç»Ÿé…ç½®
â”‚   â”œâ”€â”€ database.yaml                # æ•°æ®åº“é…ç½®
â”‚   â”œâ”€â”€ redis.yaml                   # Redisé…ç½®
â”‚   â”œâ”€â”€ models/                      # æ¨¡å‹é…ç½®
â”‚   â”‚   â”œâ”€â”€ openai.yaml
â”‚   â”‚   â”œâ”€â”€ ollama.yaml
â”‚   â”‚   â””â”€â”€ lmstudio.yaml
â”‚   â”œâ”€â”€ personalities/               # äººæ ¼é…ç½®
â”‚   â”‚   â”œâ”€â”€ default.yaml
â”‚   â”‚   â”œâ”€â”€ professional.yaml
â”‚   â”‚   â”œâ”€â”€ friendly.yaml
â”‚   â”‚   â””â”€â”€ health_assistant.yaml
â”‚   â”œâ”€â”€ tools/                       # å·¥å…·é…ç½®
â”‚   â”‚   â”œâ”€â”€ builtin.yaml
â”‚   â”‚   â””â”€â”€ mcp.yaml
â”‚   â””â”€â”€ voice/                       # è¯­éŸ³é…ç½®
â”‚       â”œâ”€â”€ stt.yaml
â”‚       â”œâ”€â”€ tts.yaml
â”‚       â””â”€â”€ realtime.yaml
â”‚
â”œâ”€â”€ tests/                           # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api/
â”‚   â”œâ”€â”€ test_engines/
â”‚   â””â”€â”€ test_core/
â”‚
â”œâ”€â”€ alembic/                         # æ•°æ®åº“è¿ç§»
â”‚   â”œâ”€â”€ versions/
â”‚   â””â”€â”€ env.py
â”‚
â”œâ”€â”€ requirements.txt                 # Pythonä¾èµ–
â”œâ”€â”€ requirements-dev.txt             # å¼€å‘ä¾èµ–
â”œâ”€â”€ Dockerfile                       # Dockeré•œåƒ
â”œâ”€â”€ .env.example                     # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â””â”€â”€ README.md
```

## 2. æ ¸å¿ƒæ¨¡å—è¯¦ç»†è®¾è®¡

### 2.1 äººæ ¼ç³»ç»Ÿ (Personality System)

#### 2.1.1 è®¾è®¡ç›®æ ‡

äººæ ¼ç³»ç»Ÿæ˜¯CozyChatçš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒå°†AIã€è®°å¿†ã€å·¥å…·ã€è¯­éŸ³ç»Ÿä¸€ç®¡ç†ï¼ŒçœŸæ­£æ¨¡æ‹Ÿä¸€ä¸ª"äºº"ä½œä¸ºäº¤äº’ä¸»ä½“ã€‚

#### 2.1.2 äººæ ¼é…ç½®ç»“æ„

```yaml
# config/personalities/health_assistant.yaml
personality:
  id: "health_assistant"
  name: "å¥åº·åŠ©æ‰‹å°ç ”"
  version: "1.0.0"
  description: "ä¸“ä¸šçš„å¥åº·å’¨è¯¢åŠ©æ‰‹"
  
  # äººæ ¼ç‰¹å¾
  traits:
    friendliness: 0.8      # å‹å¥½åº¦ 0-1
    formality: 0.6         # æ­£å¼åº¦ 0-1
    humor: 0.4             # å¹½é»˜æ„Ÿ 0-1
    empathy: 0.9           # å…±æƒ…èƒ½åŠ› 0-1
  
  # AIå¼•æ“é…ç½®
  ai:
    provider: "openai"     # openai / ollama / lmstudio
    model: "gpt-4"
    temperature: 0.7
    max_tokens: 4096
    system_prompt: |
      ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¥åº·å’¨è¯¢åŠ©æ‰‹ï¼Œåå­—å«å°ç ”ã€‚
      ä½ çš„èŒè´£æ˜¯å¸®åŠ©ç”¨æˆ·è§£ç­”å¥åº·ç›¸å…³é—®é¢˜ï¼Œæä¾›ä¸“ä¸šå»ºè®®ã€‚
      ä½ åº”è¯¥å‹å¥½ã€è€å¿ƒï¼Œå¹¶ä¸”å¯Œæœ‰åŒç†å¿ƒã€‚
    
    # Tokené¢„ç®—ç®¡ç†
    token_budget:
      max_history_tokens: 4000
      max_memory_tokens: 1000
      max_tool_tokens: 500
  
  # è®°å¿†é…ç½®
  memory:
    enabled: true
    vector_db: "chromadb"  # chromadb / qdrant
    save_mode: "both"      # both / user_only / assistant_only
    
    # è®°å¿†ç­–ç•¥
    strategy:
      user_memory:
        enabled: true
        importance_threshold: 0.5  # é‡è¦æ€§é˜ˆå€¼
        max_memories: 100
        ttl_days: 90               # è®°å¿†è¿‡æœŸæ—¶é—´
      
      ai_memory:
        enabled: true
        importance_threshold: 0.6
        max_memories: 50
        ttl_days: 30
    
    # æ£€ç´¢é…ç½®
    retrieval:
      max_results: 5
      similarity_threshold: 0.7
      timeout_seconds: 0.5
      cache_ttl_seconds: 300
  
  # å·¥å…·é…ç½®
  tools:
    enabled: true
    allowed_tools:
      - "web_search"
      - "calculator"
      - "time_tool"
      - "weather_tool"
    
    mcp_servers:
      - name: "health_mcp"
        enabled: true
        tools:
          - "search_medicine"
          - "check_symptoms"
    
    # å·¥å…·æƒé™
    permissions:
      max_concurrent_calls: 3
      timeout_seconds: 30
  
  # è¯­éŸ³é…ç½®
  voice:
    stt:
      provider: "openai"    # openai / tencent / custom
      model: "whisper-1"
      language: "zh-CN"
    
    tts:
      provider: "openai"    # openai / tencent / custom
      voice: "shimmer"
      speed: 1.0
    
    realtime:
      enabled: true
      provider: "openai"    # openai / custom
      voice: "shimmer"
      turn_detection:
        type: "server_vad"
        threshold: 0.5
  
  # ç”¨æˆ·åå¥½
  user_preferences:
    default_language: "zh-CN"
    response_style: "detailed"  # brief / detailed / conversational
    auto_tts: false
    show_reasoning: false       # æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
  
  # å…ƒæ•°æ®
  metadata:
    created_at: "2025-11-06"
    author: "CozyChat Team"
    tags: ["health", "medical", "consultation"]
    icon: "ğŸ¥"
```

#### 2.1.3 PersonalityManagerå®ç°

```python
# app/core/personality/manager.py
from typing import Dict, Optional, Any
from pathlib import Path
import yaml
from app.core.personality.models import Personality
from app.utils.logger import logger

class PersonalityManager:
    """äººæ ¼ç®¡ç†å™¨ - åŠ è½½ã€ç®¡ç†ã€åˆ‡æ¢äººæ ¼"""
    
    def __init__(self, config_dir: Path):
        self.config_dir = config_dir
        self.personalities: Dict[str, Personality] = {}
        self._load_all_personalities()
    
    def _load_all_personalities(self):
        """ä»é…ç½®ç›®å½•åŠ è½½æ‰€æœ‰äººæ ¼"""
        personality_files = self.config_dir.glob("*.yaml")
        for file in personality_files:
            try:
                personality = self._load_personality(file)
                self.personalities[personality.id] = personality
                logger.info(f"Loaded personality: {personality.name}")
            except Exception as e:
                logger.error(f"Failed to load personality from {file}: {e}")
    
    def _load_personality(self, file_path: Path) -> Personality:
        """åŠ è½½å•ä¸ªäººæ ¼é…ç½®"""
        with open(file_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return Personality.from_config(config['personality'])
    
    def get_personality(self, personality_id: str) -> Optional[Personality]:
        """è·å–æŒ‡å®šäººæ ¼"""
        return self.personalities.get(personality_id)
    
    def list_personalities(self) -> list[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨äººæ ¼"""
        return [
            {
                "id": p.id,
                "name": p.name,
                "description": p.description,
                "icon": p.metadata.get("icon"),
                "tags": p.metadata.get("tags", [])
            }
            for p in self.personalities.values()
        ]
    
    def create_personality(self, config: Dict[str, Any]) -> Personality:
        """åŠ¨æ€åˆ›å»ºäººæ ¼ï¼ˆç”¨æˆ·è‡ªå®šä¹‰äººæ ¼ï¼‰"""
        personality = Personality.from_config(config)
        self.personalities[personality.id] = personality
        
        # ä¿å­˜åˆ°é…ç½®æ–‡ä»¶
        self._save_personality(personality)
        return personality
    
    def update_personality(self, personality_id: str, updates: Dict[str, Any]) -> Personality:
        """æ›´æ–°äººæ ¼é…ç½®"""
        personality = self.get_personality(personality_id)
        if not personality:
            raise ValueError(f"Personality {personality_id} not found")
        
        personality.update(updates)
        self._save_personality(personality)
        return personality
    
    def delete_personality(self, personality_id: str):
        """åˆ é™¤äººæ ¼"""
        if personality_id in self.personalities:
            del self.personalities[personality_id]
            # åˆ é™¤é…ç½®æ–‡ä»¶
            file_path = self.config_dir / f"{personality_id}.yaml"
            if file_path.exists():
                file_path.unlink()
    
    def _save_personality(self, personality: Personality):
        """ä¿å­˜äººæ ¼é…ç½®åˆ°æ–‡ä»¶"""
        file_path = self.config_dir / f"{personality.id}.yaml"
        with open(file_path, 'w', encoding='utf-8') as f:
            yaml.dump(
                {"personality": personality.to_config()},
                f,
                allow_unicode=True,
                sort_keys=False
            )
```

### 2.2 AIå¼•æ“ (AI Engine)

#### 2.2.1 è®¾è®¡ç›®æ ‡

æ”¯æŒå¤šç§å¤§æ¨¡å‹æä¾›å•†ï¼Œç»Ÿä¸€æ¥å£ï¼Œå¯æ’æ‹”æ¶æ„ã€‚

#### 2.2.2 AIå¼•æ“åŸºç±»

```python
# app/engines/ai/base.py
from abc import ABC, abstractmethod
from typing import AsyncIterator, Dict, Any, List, Optional
from enum import Enum

class ModelProvider(Enum):
    OPENAI = "openai"
    OLLAMA = "ollama"
    LMSTUDIO = "lmstudio"

class AIEngineBase(ABC):
    """AIå¼•æ“åŸºç±» - å®šä¹‰ç»Ÿä¸€æ¥å£"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.provider = self._get_provider()
        self.model = config.get("model")
        self.temperature = config.get("temperature", 0.7)
        self.max_tokens = config.get("max_tokens", 4096)
    
    @abstractmethod
    def _get_provider(self) -> ModelProvider:
        """è¿”å›æä¾›å•†ç±»å‹"""
        pass
    
    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[Dict] | Dict:
        """
        èŠå¤©å®Œæˆæ¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            stream: æ˜¯å¦æµå¼è¿”å›
            tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
        
        Returns:
            æµå¼: AsyncIterator[Dict] è¿”å›å¢é‡æ¶ˆæ¯
            éæµå¼: Dict è¿”å›å®Œæ•´å“åº”
        """
        pass
    
    @abstractmethod
    async def count_tokens(self, messages: List[Dict[str, str]]) -> int:
        """è®¡ç®—æ¶ˆæ¯çš„tokenæ•°é‡"""
        pass
    
    @abstractmethod
    async def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        pass
    
    def supports_function_calling(self) -> bool:
        """æ˜¯å¦æ”¯æŒå‡½æ•°è°ƒç”¨"""
        return False
    
    def supports_streaming(self) -> bool:
        """æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º"""
        return True
```

#### 2.2.3 OpenAIå¼•æ“å®ç°

```python
# app/engines/ai/openai_engine.py
from openai import AsyncOpenAI
from typing import AsyncIterator, Dict, Any, List, Optional
from app.engines.ai.base import AIEngineBase, ModelProvider
from app.utils.logger import logger

class OpenAIEngine(AIEngineBase):
    """OpenAIæ¨¡å‹å¼•æ“å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.client = AsyncOpenAI(
            api_key=config.get("api_key"),
            base_url=config.get("base_url", "https://api.openai.com/v1"),
            timeout=config.get("timeout", 60.0)
        )
    
    def _get_provider(self) -> ModelProvider:
        return ModelProvider.OPENAI
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[Dict] | Dict:
        """OpenAIèŠå¤©å®Œæˆ"""
        
        params = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", self.temperature),
            "max_tokens": kwargs.get("max_tokens", self.max_tokens),
            "stream": stream
        }
        
        if tools:
            params["tools"] = tools
            params["tool_choice"] = kwargs.get("tool_choice", "auto")
        
        if stream:
            return self._stream_completion(params)
        else:
            response = await self.client.chat.completions.create(**params)
            return {
                "role": "assistant",
                "content": response.choices[0].message.content,
                "finish_reason": response.choices[0].finish_reason,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
    
    async def _stream_completion(self, params: Dict) -> AsyncIterator[Dict]:
        """æµå¼å®Œæˆ"""
        stream = await self.client.chat.completions.create(**params)
        async for chunk in stream:
            delta = chunk.choices[0].delta
            if delta.content:
                yield {
                    "content": delta.content,
                    "finish_reason": chunk.choices[0].finish_reason
                }
    
    async def count_tokens(self, messages: List[Dict[str, str]]) -> int:
        """ä½¿ç”¨tiktokenè®¡ç®—token"""
        import tiktoken
        try:
            encoding = tiktoken.encoding_for_model(self.model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")
        
        num_tokens = 0
        for message in messages:
            num_tokens += 4  # æ¯æ¡æ¶ˆæ¯çš„å›ºå®šå¼€é”€
            for key, value in message.items():
                num_tokens += len(encoding.encode(str(value)))
        num_tokens += 2  # å›å¤çš„å›ºå®šå¼€é”€
        return num_tokens
    
    async def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "provider": self.provider.value,
            "model": self.model,
            "supports_function_calling": True,
            "supports_streaming": True,
            "max_tokens": self.max_tokens
        }
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            await self.client.models.list()
            return True
        except Exception as e:
            logger.error(f"OpenAI health check failed: {e}")
            return False
    
    def supports_function_calling(self) -> bool:
        return True
```

#### 2.2.4 Ollamaå¼•æ“å®ç°

```python
# app/engines/ai/ollama_engine.py
import httpx
from typing import AsyncIterator, Dict, Any, List, Optional
from app.engines.ai.base import AIEngineBase, ModelProvider
from app.utils.logger import logger

class OllamaEngine(AIEngineBase):
    """Ollamaæœ¬åœ°æ¨¡å‹å¼•æ“å®ç°"""
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self.base_url = config.get("base_url", "http://localhost:11434")
        self.client = httpx.AsyncClient(base_url=self.base_url, timeout=60.0)
    
    def _get_provider(self) -> ModelProvider:
        return ModelProvider.OLLAMA
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        stream: bool = False,
        tools: Optional[List[Dict]] = None,
        **kwargs
    ) -> AsyncIterator[Dict] | Dict:
        """OllamaèŠå¤©å®Œæˆ"""
        
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": stream,
            "options": {
                "temperature": kwargs.get("temperature", self.temperature),
                "num_predict": kwargs.get("max_tokens", self.max_tokens)
            }
        }
        
        if stream:
            return self._stream_completion(payload)
        else:
            response = await self.client.post("/api/chat", json=payload)
            response.raise_for_status()
            data = response.json()
            return {
                "role": "assistant",
                "content": data["message"]["content"],
                "finish_reason": "stop",
                "usage": {
                    "prompt_tokens": data.get("prompt_eval_count", 0),
                    "completion_tokens": data.get("eval_count", 0),
                    "total_tokens": data.get("prompt_eval_count", 0) + data.get("eval_count", 0)
                }
            }
    
    async def _stream_completion(self, payload: Dict) -> AsyncIterator[Dict]:
        """æµå¼å®Œæˆ"""
        async with self.client.stream("POST", "/api/chat", json=payload) as response:
            response.raise_for_status()
            async for line in response.aiter_lines():
                if line:
                    import json
                    data = json.loads(line)
                    if "message" in data:
                        yield {
                            "content": data["message"]["content"],
                            "finish_reason": "stop" if data.get("done") else None
                        }
    
    async def count_tokens(self, messages: List[Dict[str, str]]) -> int:
        """ä¼°ç®—tokenæ•°é‡ï¼ˆOllamaæ²¡æœ‰æä¾›tokenè®¡æ•°APIï¼‰"""
        # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦=1token
        total_chars = sum(len(msg.get("content", "")) for msg in messages)
        return int(total_chars / 3)  # ç²—ç•¥ä¼°ç®—
    
    async def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        response = await self.client.post("/api/show", json={"name": self.model})
        data = response.json()
        return {
            "provider": self.provider.value,
            "model": self.model,
            "supports_function_calling": False,
            "supports_streaming": True,
            "max_tokens": self.max_tokens,
            "details": data
        }
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            response = await self.client.get("/api/tags")
            return response.status_code == 200
        except Exception as e:
            logger.error(f"Ollama health check failed: {e}")
            return False
```

#### 2.2.5 å¼•æ“å·¥å‚

```python
# app/engines/ai/factory.py
from typing import Dict, Any
from app.engines.ai.base import AIEngineBase, ModelProvider
from app.engines.ai.openai_engine import OpenAIEngine
from app.engines.ai.ollama_engine import OllamaEngine
from app.engines.ai.lmstudio_engine import LMStudioEngine

class AIEngineFactory:
    """AIå¼•æ“å·¥å‚ - æ ¹æ®é…ç½®åˆ›å»ºå¯¹åº”çš„å¼•æ“"""
    
    _engine_map = {
        ModelProvider.OPENAI: OpenAIEngine,
        ModelProvider.OLLAMA: OllamaEngine,
        ModelProvider.LMSTUDIO: LMStudioEngine
    }
    
    @classmethod
    def create_engine(cls, provider: str, config: Dict[str, Any]) -> AIEngineBase:
        """
        åˆ›å»ºAIå¼•æ“å®ä¾‹
        
        Args:
            provider: æä¾›å•†åç§°ï¼ˆopenai/ollama/lmstudioï¼‰
            config: å¼•æ“é…ç½®
        
        Returns:
            AIEngineBase: å¼•æ“å®ä¾‹
        """
        provider_enum = ModelProvider(provider.lower())
        engine_class = cls._engine_map.get(provider_enum)
        
        if not engine_class:
            raise ValueError(f"Unsupported AI provider: {provider}")
        
        return engine_class(config)
    
    @classmethod
    def register_engine(cls, provider: ModelProvider, engine_class: type):
        """æ³¨å†Œè‡ªå®šä¹‰å¼•æ“"""
        cls._engine_map[provider] = engine_class
```

---

**ï¼ˆç»­ä¸‹ä¸€éƒ¨åˆ†ï¼šè®°å¿†ç³»ç»Ÿã€å·¥å…·ç³»ç»Ÿã€è¯­éŸ³ç³»ç»Ÿè®¾è®¡ï¼‰**

ç”±äºæ–‡æ¡£è¾ƒé•¿ï¼Œæˆ‘å°†ç»§ç»­åˆ›å»ºå…¶ä»–è®¾è®¡æ–‡æ¡£ã€‚è¦ç»§ç»­å—ï¼Ÿ

