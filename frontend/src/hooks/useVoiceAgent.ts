import { useState, useRef, useCallback, useEffect } from 'react';
import { useQuery } from '@tanstack/react-query';
import { RealtimeAgent, RealtimeSession, OpenAIRealtimeWebRTC } from '@openai/agents/realtime';
import { configApi } from '@/services/config';
import { personalityApi } from '@/services/personality';
import type { OpenAIConfig } from '@/services/config';

/**
 * Voice Agent Hookè¿”å›å€¼
 */
export interface UseVoiceAgentReturn {
  /** æ˜¯å¦å·²è¿æ¥ */
  isConnected: boolean;
  /** æ˜¯å¦æ­£åœ¨é€šè¯ */
  isCalling: boolean;
  /** é”™è¯¯ä¿¡æ¯ */
  error: string | null;
  /** ç”¨æˆ·éŸ³é¢‘é¢‘ç‡æ•°æ®ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰ */
  userFrequencyData: Uint8Array | null;
  /** åŠ©æ‰‹éŸ³é¢‘é¢‘ç‡æ•°æ®ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰ */
  assistantFrequencyData: Uint8Array | null;
  /** è¿æ¥ Voice Agent */
  connect: () => Promise<void>;
  /** æ–­å¼€è¿æ¥ */
  disconnect: () => void;
  /** å¼€å§‹é€šè¯ */
  startCall: () => Promise<void>;
  /** ç»“æŸé€šè¯ */
  endCall: () => Promise<void>;
}

/**
 * Voice Agent Hook
 *
 * ä½¿ç”¨ OpenAI Agents SDK çš„ Realtime API å®ç°è¯­éŸ³é€šè¯åŠŸèƒ½ã€‚
 * ç”±äº oneapi.naivehero.top æ˜¯ api.openai.com çš„å®Œæ•´é•œåƒï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ OpenAI SDKã€‚
 *
 * @param sessionId - ä¼šè¯ID
 * @param personalityId - äººæ ¼ID
 * @param callbacks - å›è°ƒå‡½æ•°
 * @returns Voice Agent Hookè¿”å›å€¼
 */
export const useVoiceAgent = (
  _sessionId?: string,
  personalityId?: string,
  callbacks?: {
    onUserTranscript?: (text: string) => void;
    onAssistantTranscript?: (text: string) => void;
  }
): UseVoiceAgentReturn => {
  const [isConnected, setIsConnected] = useState(false);
  const [isCalling, setIsCalling] = useState(false);
  const [error, setError] = useState<string | null>(null);
  
  const sessionRef = useRef<RealtimeSession | null>(null);
  const configRef = useRef<OpenAIConfig | null>(null);
  const isCallingRef = useRef(false);
  
  // éŸ³é¢‘æµå’Œå…ƒç´ å¼•ç”¨ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
  const userMediaStreamRef = useRef<MediaStream | null>(null);
  const assistantAudioElementRef = useRef<HTMLAudioElement | null>(null);
  
  // éŸ³é¢‘å¯è§†åŒ–ç›¸å…³
  const userAnalyserRef = useRef<AnalyserNode | null>(null);
  const assistantAnalyserRef = useRef<AnalyserNode | null>(null);
  const assistantSourceRef = useRef<MediaElementAudioSourceNode | null>(null);
  const assistantAudioContextRef = useRef<AudioContext | null>(null);
  const [userFrequencyData, setUserFrequencyData] = useState<Uint8Array | null>(null);
  const [assistantFrequencyData, setAssistantFrequencyData] = useState<Uint8Array | null>(null);
  const userAnimationFrameRef = useRef<number | null>(null);
  const assistantAnimationFrameRef = useRef<number | null>(null);
  
  // è·å– personality é…ç½®
  const { data: personality } = useQuery({
    queryKey: ['personality', personalityId],
    queryFn: () => personalityApi.getPersonality(personalityId!),
    enabled: !!personalityId,
  });

  /**
   * åŠ è½½é…ç½®
   */
  const loadConfig = useCallback(async (): Promise<OpenAIConfig> => {
    if (configRef.current) {
      return configRef.current;
    }
    
    const config = await configApi.getOpenAIConfig();
    configRef.current = config;
    return config;
  }, []);

  /**
   * åˆå§‹åŒ–ç”¨æˆ·éŸ³é¢‘å¯è§†åŒ–
   */
  const initUserAudioVisualization = useCallback(async (stream: MediaStream) => {
    try {
      // æ£€æŸ¥ AudioContext çŠ¶æ€
      let audioContext: AudioContext;
      try {
        audioContext = new AudioContext({ sampleRate: 24000 });
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }
      } catch (e) {
        console.error('åˆ›å»º AudioContext å¤±è´¥:', e);
        return;
      }
      
      const source = audioContext.createMediaStreamSource(stream);
      
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      analyser.smoothingTimeConstant = 0.3;
      userAnalyserRef.current = analyser;
      
      source.connect(analyser);
      
      // å¯åŠ¨ç”¨æˆ·éŸ³é¢‘å¯è§†åŒ–
      const updateUserAudioVisualization = () => {
        if (!userAnalyserRef.current || !isCallingRef.current) {
          return;
        }
        
        try {
          const bufferLength = userAnalyserRef.current.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          userAnalyserRef.current.getByteFrequencyData(dataArray);
          
          setUserFrequencyData(dataArray);
          
          userAnimationFrameRef.current = requestAnimationFrame(() => {
            updateUserAudioVisualization();
          }) as any;
        } catch (err) {
          console.error('æ›´æ–°ç”¨æˆ·éŸ³é¢‘å¯è§†åŒ–å¤±è´¥:', err);
        }
      };
      
      // å»¶è¿Ÿå¯åŠ¨ï¼Œç¡®ä¿ isCallingRef å·²è®¾ç½®
      setTimeout(() => {
        updateUserAudioVisualization();
      }, 200);
    } catch (err: any) {
      console.error('åˆå§‹åŒ–ç”¨æˆ·éŸ³é¢‘å¯è§†åŒ–å¤±è´¥:', err);
    }
  }, []);

  /**
   * åˆå§‹åŒ–åŠ©æ‰‹éŸ³é¢‘å¯è§†åŒ–
   */
  const initAssistantAudioVisualization = useCallback((audioElement: HTMLAudioElement) => {
    try {
      // æ¸…ç†ä¹‹å‰çš„è¿æ¥ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
      if (assistantSourceRef.current) {
        try {
          assistantSourceRef.current.disconnect();
        } catch (e) {
          // å¿½ç•¥æ–­å¼€è¿æ¥é”™è¯¯
        }
        assistantSourceRef.current = null;
      }
      
      if (assistantAudioContextRef.current) {
        try {
          assistantAudioContextRef.current.close();
        } catch (e) {
          // å¿½ç•¥å…³é—­é”™è¯¯
        }
        assistantAudioContextRef.current = null;
      }
      
      // æ£€æŸ¥ AudioContext çŠ¶æ€
      let audioContext: AudioContext;
      try {
        audioContext = new AudioContext({ sampleRate: 24000 });
        assistantAudioContextRef.current = audioContext;
        if (audioContext.state === 'suspended') {
          audioContext.resume();
        }
      } catch (e) {
        console.error('åˆ›å»ºåŠ©æ‰‹ AudioContext å¤±è´¥:', e);
        return;
      }
      
      // ä¼˜å…ˆä½¿ç”¨ srcObject çš„ MediaStreamï¼ˆæ›´å¯é ï¼Œä¸ä¼šå‡ºç°"already connected"é”™è¯¯ï¼‰
      // æ³¨æ„ï¼šä¸è¦åŒæ—¶ä½¿ç”¨ MediaStreamSource å’Œ MediaElementSourceï¼Œä¼šå¯¼è‡´é‡å¤æ’­æ”¾
      let source: MediaElementAudioSourceNode | MediaStreamAudioSourceNode;
      
      if (audioElement.srcObject instanceof MediaStream) {
        // å¦‚æœ audioElement æœ‰ srcObjectï¼ˆMediaStreamï¼‰ï¼Œç›´æ¥ä½¿ç”¨å®ƒ
        try {
          const streamSource = audioContext.createMediaStreamSource(audioElement.srcObject);
          assistantSourceRef.current = streamSource as any;
          source = streamSource;
        } catch (e: any) {
          console.error('ä» MediaStream åˆ›å»ºéŸ³é¢‘æºå¤±è´¥:', e);
          throw e;
        }
      } else {
        // å¦‚æœæ²¡æœ‰ srcObjectï¼Œå°è¯•ä» audioElement åˆ›å»º MediaElementSource
        try {
          source = audioContext.createMediaElementSource(audioElement);
          assistantSourceRef.current = source;
        } catch (e: any) {
          if (e.name === 'InvalidStateError' && e.message.includes('already connected')) {
            // éŸ³é¢‘å…ƒç´ å·²è¢«è¿æ¥ï¼Œè·³è¿‡å¯è§†åŒ–ï¼ˆé¿å…é‡å¤æ’­æ”¾ï¼‰
            return;
          } else {
            throw e;
          }
        }
      }
      
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 512;
      analyser.smoothingTimeConstant = 0.1;
      analyser.minDecibels = -90;
      analyser.maxDecibels = -10;
      assistantAnalyserRef.current = analyser;
      
      source.connect(analyser);
      analyser.connect(audioContext.destination);
      
      // å¯åŠ¨åŠ©æ‰‹éŸ³é¢‘å¯è§†åŒ–
      const updateAssistantAudioVisualization = () => {
        if (!assistantAnalyserRef.current || !isCallingRef.current) {
          return;
        }
        
        try {
          const bufferLength = assistantAnalyserRef.current.frequencyBinCount;
          const dataArray = new Uint8Array(bufferLength);
          assistantAnalyserRef.current.getByteFrequencyData(dataArray);
          
          setAssistantFrequencyData(dataArray);
          
          assistantAnimationFrameRef.current = requestAnimationFrame(() => {
            updateAssistantAudioVisualization();
          }) as any;
        } catch (err) {
          console.error('æ›´æ–°åŠ©æ‰‹éŸ³é¢‘å¯è§†åŒ–å¤±è´¥:', err);
          // å¦‚æœå‡ºé”™ï¼Œåœæ­¢æ›´æ–°
          if (assistantAnimationFrameRef.current) {
            cancelAnimationFrame(assistantAnimationFrameRef.current);
            assistantAnimationFrameRef.current = null;
          }
        }
      };
      
      // ç«‹å³å¯åŠ¨å¯è§†åŒ–å¾ªç¯
      if (isCallingRef.current && assistantAnalyserRef.current) {
        updateAssistantAudioVisualization();
      } else {
        // å»¶è¿Ÿå¯åŠ¨ï¼Œç­‰å¾…æ¡ä»¶æ»¡è¶³
        setTimeout(() => {
          if (isCallingRef.current && assistantAnalyserRef.current) {
            updateAssistantAudioVisualization();
          }
      }, 200);
      }
    } catch (err: any) {
      console.error('åˆå§‹åŒ–åŠ©æ‰‹éŸ³é¢‘å¯è§†åŒ–å¤±è´¥:', err);
    }
  }, []);

  /**
   * è¿æ¥ Voice Agent
   */
  const connect = useCallback(async () => {
    try {
      setError(null);
      
      // è·å–é…ç½®
      const config = await loadConfig();
      
      // è·å– ephemeral client key (ä¸´æ—¶å¯†é’¥)
      const realtimeToken = await configApi.getRealtimeToken();
      
      // è·å–å…¨å±€é»˜è®¤é…ç½®ï¼ˆæ¥è‡ª realtime.yamlï¼‰
      const globalConfig = await configApi.getRealtimeConfig();
      
      // è·å– personality é…ç½®
      const personalityConfig = (personality as any)?.config || {};
      const voiceConfig = personalityConfig?.voice || {};
      const personalityRealtimeConfig = voiceConfig?.realtime || {};
      
      // åˆå¹¶é…ç½®ï¼špersonality é…ç½® > å…¨å±€é…ç½® > ä»£ç é»˜è®¤å€¼
      const voice = personalityRealtimeConfig.voice || globalConfig.voice || 'shimmer';
      const instructions = personalityRealtimeConfig.instructions || personalityConfig?.ai?.system_prompt || 'You are a helpful assistant.';
      
      console.log('ğŸ™ï¸ Realtime Voice é…ç½®:', {
        global: globalConfig.voice,
        personality: personalityRealtimeConfig.voice,
        final: voice,
      });
      
      // åˆ›å»º RealtimeAgent
      const agent = new RealtimeAgent({
        name: 'cozychat-agent',
        instructions: instructions,
        voice: voice,
      });
      
      // åˆ›å»ºç”¨æˆ·éŸ³é¢‘æµï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
      // æˆ‘ä»¬éœ€è¦è‡ªå·±åˆ›å»º mediaStreamï¼Œè¿™æ ·å¯ä»¥ä»å®ƒè·å–éŸ³é¢‘æ•°æ®ç”¨äºå¯è§†åŒ–
      const userMediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: 24000,
          echoCancellation: true,
          noiseSuppression: true,
        }
      });
      userMediaStreamRef.current = userMediaStream;
      
      // åˆ›å»ºåŠ©æ‰‹éŸ³é¢‘å…ƒç´ ï¼ˆä»…ç”¨äºå¯è§†åŒ–ï¼Œä¸è‡ªåŠ¨æ’­æ”¾ï¼‰
      // æ³¨æ„ï¼šWebRTC transport ä¼šè‡ªåŠ¨å¤„ç†éŸ³é¢‘æ’­æ”¾ï¼Œæˆ‘ä»¬åªéœ€è¦å¯è§†åŒ–
      // å…³é”®ï¼šå¿…é¡»é™éŸ³ï¼Œå¦åˆ™ä¼šå’Œ transport çš„æ’­æ”¾é‡å ï¼Œå¯¼è‡´å›å£°
      const assistantAudioElement = new Audio();
      assistantAudioElement.autoplay = false; // ç¦ç”¨è‡ªåŠ¨æ’­æ”¾
      assistantAudioElement.muted = true; // é™éŸ³ï¼åªç”¨äºå¯è§†åŒ–ï¼Œä¸ç”¨äºæ’­æ”¾ï¼ˆé¿å…ä¸ transport æ’­æ”¾é‡å ï¼‰
      assistantAudioElementRef.current = assistantAudioElement;
      
      // åˆ›å»º WebRTC ä¼ è¾“å±‚ï¼ˆæµè§ˆå™¨ç¯å¢ƒï¼‰
      // ä¼ é€’æˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„ mediaStream å’Œ audioElementï¼Œä»¥ä¾¿ç”¨äºå¯è§†åŒ–
      // æ³¨æ„ï¼šbaseUrl éœ€è¦æ˜¯å®Œæ•´çš„ç«¯ç‚¹ URLï¼ŒåŒ…æ‹¬ /v1/realtime/calls è·¯å¾„
      // SDK ä¸ä¼šè‡ªåŠ¨æ·»åŠ è·¯å¾„ï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šå®Œæ•´ URL
      let baseUrl = config.base_url;
      if (baseUrl.endsWith('/v1')) {
        baseUrl = baseUrl.slice(0, -3);
      } else if (baseUrl.endsWith('/v1/')) {
        baseUrl = baseUrl.slice(0, -4);
      }
      // ç¡®ä¿ baseUrl ä¸ä»¥ / ç»“å°¾
      baseUrl = baseUrl.replace(/\/$/, '');
      // æ·»åŠ  /v1/realtime/calls è·¯å¾„ï¼ˆWebRTC ç«¯ç‚¹ï¼‰
      const webrtcEndpoint = `${baseUrl}/v1/realtime/calls`;
      
      const transport = new OpenAIRealtimeWebRTC({
        baseUrl: webrtcEndpoint, // ä½¿ç”¨å®Œæ•´çš„ç«¯ç‚¹ URLï¼ˆä¾‹å¦‚ï¼šhttps://oneapi.naivehero.top/v1/realtime/callsï¼‰
        // ä¸ä½¿ç”¨ useInsecureApiKeyï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨æœ‰ ephemeral key
        mediaStream: userMediaStream, // ä½¿ç”¨æˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„éŸ³é¢‘æµ
        audioElement: assistantAudioElement, // ä½¿ç”¨æˆ‘ä»¬è‡ªå·±åˆ›å»ºçš„éŸ³é¢‘å…ƒç´ 
      });
      
      // åˆ›å»º RealtimeSession
      // æ³¨æ„ï¼šè½¬å½•é…ç½®å·²ç»åœ¨åç«¯åˆ›å»º ephemeral token æ—¶å®Œæˆ
      const session = new RealtimeSession(agent, {
        apiKey: realtimeToken.token, // ä½¿ç”¨åç«¯ç”Ÿæˆçš„ ephemeral keyï¼ˆå·²åŒ…å«è½¬å½•é…ç½®ï¼‰
        transport: transport, // ä½¿ç”¨è‡ªå®šä¹‰çš„ WebRTC ä¼ è¾“å±‚
        model: realtimeToken.model, // ä½¿ç”¨åç«¯è¿”å›çš„æ¨¡å‹åç§°
      });
      
      // ä¿å­˜ webrtcEndpoint åˆ° session çš„æŸä¸ªåœ°æ–¹ï¼Œä»¥ä¾¿åœ¨ connect æ—¶ä½¿ç”¨
      (session as any).__webrtcEndpoint = webrtcEndpoint;
      
      // ========== æ­£ç¡®çš„äº‹ä»¶ç›‘å¬æ–¹å¼ ==========
      // æ ¹æ®ç¤¾åŒºè®¨è®ºï¼šhttps://community.openai.com/t/input-audio-transcription-in-realtime-api/1007401/5
      // æ­£ç¡®çš„äº‹ä»¶åæ˜¯ï¼šconversation.item.input_audio_transcription.completed
      
      // 1. ç”¨æˆ·è¯­éŸ³è½¬æ–‡æœ¬äº‹ä»¶ï¼ˆå®Œæˆï¼‰
      (session as any).on('conversation.item.input_audio_transcription.completed', (event: any) => {
        const transcript = event?.transcript;
        if (transcript && typeof transcript === 'string' && transcript.trim() && callbacks?.onUserTranscript) {
          callbacks.onUserTranscript(transcript);
        }
      });
      
      // 2. ä» history_added å’Œ history_updated æå–æ–‡æœ¬
      // ç”¨äºå»é‡çš„ Setï¼ˆå­˜å‚¨å·²å¤„ç†çš„æ¶ˆæ¯IDå’Œæ–‡æœ¬å†…å®¹ï¼‰
      const processedMessageIds = new Set<string>();
      const processedTexts = new Set<string>(); // å­˜å‚¨å·²å¤„ç†çš„æ–‡æœ¬å†…å®¹ï¼ˆæ¶ˆæ¯ID:æ–‡æœ¬å†…å®¹ï¼‰
      
      // æå–ç”¨æˆ·è½¬å½•æ–‡æœ¬çš„è¾…åŠ©å‡½æ•°
      const extractUserTranscript = (item: any): string | null => {
        // 1. é¦–å…ˆæ£€æŸ¥ item çš„ç›´æ¥å­—æ®µ
        if (item.transcript && typeof item.transcript === 'string' && item.transcript.trim()) {
          return item.transcript.trim();
        }
        if (item.input_audio_transcript && typeof item.input_audio_transcript === 'string' && item.input_audio_transcript.trim()) {
          return item.input_audio_transcript.trim();
        }
        
        // 2. æ£€æŸ¥ content æ•°ç»„ï¼ˆè½¬å½•æ–‡æœ¬åœ¨è¿™é‡Œï¼‰
        if (Array.isArray(item.content)) {
          for (const c of item.content) {
            // ä¼˜å…ˆæ£€æŸ¥ input_audio ç±»å‹ï¼ˆè¿™æ˜¯ç”¨æˆ·è¯­éŸ³è¾“å…¥ï¼‰
            if (c.type === 'input_audio') {
              if (c.transcript && typeof c.transcript === 'string' && c.transcript.trim()) {
                return c.transcript.trim();
              }
              if (c.input_audio_transcript && typeof c.input_audio_transcript === 'string' && c.input_audio_transcript.trim()) {
                return c.input_audio_transcript.trim();
              }
              if (c.text && typeof c.text === 'string' && c.text.trim()) {
                return c.text.trim();
              }
            }
            // æ£€æŸ¥ä»»ä½•åŒ…å« transcript çš„é¡¹ï¼ˆå¤‡ç”¨ï¼‰
            if (c.transcript && typeof c.transcript === 'string' && c.transcript.trim()) {
              return c.transcript.trim();
            }
            // æ£€æŸ¥ text ç±»å‹ï¼ˆæŸäº›æƒ…å†µä¸‹è½¬å½•å¯èƒ½ä»¥ text å½¢å¼å­˜åœ¨ï¼‰
            if (c.type === 'text' && c.text && typeof c.text === 'string' && c.text.trim()) {
              return c.text.trim();
            }
          }
        }
        
        // 3. å¦‚æœ content æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›ï¼ˆå¤‡ç”¨ï¼‰
        if (typeof item.content === 'string' && item.content.trim()) {
          return item.content.trim();
        }
        
        return null;
      };
      
      // æå–åŠ©æ‰‹æ–‡æœ¬çš„è¾…åŠ©å‡½æ•°
      const extractAssistantText = (item: any): string | null => {
        // æ£€æŸ¥ content æ•°ç»„
        if (Array.isArray(item.content)) {
          for (const c of item.content) {
            if (c.type === 'text' && c.text && typeof c.text === 'string') {
              return c.text.trim();
            }
            if (c.type === 'output_audio' && c.transcript && typeof c.transcript === 'string') {
              return c.transcript.trim();
            }
          }
        }
        
        // æ£€æŸ¥ç›´æ¥å­—æ®µ
        if (item.text && typeof item.text === 'string') {
          return item.text.trim();
        }
        
        return null;
      };
      
      session.on('history_added', (item: any) => {
        if (item.type === 'message') {
          const messageId = item.itemId || item.id;
          if (!messageId) {
            return; // æ²¡æœ‰æœ‰æ•ˆçš„æ¶ˆæ¯IDï¼Œè·³è¿‡
          }
          
          // æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¿™ä¸ªæ¶ˆæ¯ID
          if (processedMessageIds.has(messageId)) {
            return;
          }
          
          if (item.role === 'user') {
            const transcript = extractUserTranscript(item);
            if (transcript && callbacks?.onUserTranscript) {
              const textKey = `${messageId}:${transcript}`;
              if (!processedTexts.has(textKey)) {
                processedMessageIds.add(messageId);
                processedTexts.add(textKey);
              callbacks.onUserTranscript(transcript);
              }
            }
          } else if (item.role === 'assistant') {
            const text = extractAssistantText(item);
            if (text && callbacks?.onAssistantTranscript) {
              const textKey = `${messageId}:${text}`;
              if (!processedTexts.has(textKey)) {
                processedMessageIds.add(messageId);
                processedTexts.add(textKey);
              callbacks.onAssistantTranscript(text);
              }
            }
          }
        }
      });
      
      session.on('history_updated', (history: any[]) => {
        // éå†æ‰€æœ‰æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„è½¬å½•æ–‡æœ¬
        history.forEach((item: any) => {
          if (item.type === 'message') {
            const messageId = item.itemId || item.id;
            if (!messageId) {
              return; // æ²¡æœ‰æœ‰æ•ˆçš„æ¶ˆæ¯IDï¼Œè·³è¿‡
            }
            
            if (item.role === 'user') {
              const transcript = extractUserTranscript(item);
              if (transcript) {
                // ä½¿ç”¨æ¶ˆæ¯IDå’Œæ–‡æœ¬å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†
                const textKey = `${messageId}:${transcript}`;
                
                // å¦‚æœä¹‹å‰æ²¡æœ‰å¤„ç†è¿‡è¿™ä¸ªæ–‡æœ¬
                if (!processedTexts.has(textKey) && callbacks?.onUserTranscript) {
                  processedMessageIds.add(messageId);
                  processedTexts.add(textKey);
                callbacks.onUserTranscript(transcript);
                }
              }
            } else if (item.role === 'assistant') {
              const text = extractAssistantText(item);
              if (text) {
                // ä½¿ç”¨æ¶ˆæ¯IDå’Œæ–‡æœ¬å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†
                const textKey = `${messageId}:${text}`;
                
                // å¦‚æœä¹‹å‰æ²¡æœ‰å¤„ç†è¿‡è¿™ä¸ªæ–‡æœ¬
                if (!processedTexts.has(textKey) && callbacks?.onAssistantTranscript) {
                  processedMessageIds.add(messageId);
                  processedTexts.add(textKey);
                callbacks.onAssistantTranscript(text);
                }
              }
            }
          }
        });
      });
      
      sessionRef.current = session;
      setIsConnected(true);
      
      console.log('Voice Agent è¿æ¥æˆåŠŸ');
    } catch (err: any) {
      console.error('è¿æ¥ Voice Agent å¤±è´¥:', err);
      setError(err.message || 'è¿æ¥å¤±è´¥');
      throw err;
    }
  }, [loadConfig, personality, callbacks]);

  /**
   * æ–­å¼€è¿æ¥
   */
  const disconnect = useCallback(() => {
    try {
      if (sessionRef.current) {
        sessionRef.current.close(); // ä½¿ç”¨ close() æ–¹æ³•æ–­å¼€è¿æ¥
        sessionRef.current = null;
      }
      
      // åœæ­¢ç”¨æˆ·éŸ³é¢‘æµ
      if (userMediaStreamRef.current) {
        userMediaStreamRef.current.getTracks().forEach(track => track.stop());
        userMediaStreamRef.current = null;
      }
      
      // åœæ­¢åŠ©æ‰‹éŸ³é¢‘å…ƒç´ 
      if (assistantAudioElementRef.current) {
        assistantAudioElementRef.current.pause();
        assistantAudioElementRef.current.src = '';
        assistantAudioElementRef.current = null;
      }
      
      // åœæ­¢éŸ³é¢‘å¯è§†åŒ–
      if (userAnimationFrameRef.current) {
        clearTimeout(userAnimationFrameRef.current as any);
        userAnimationFrameRef.current = null;
      }
      if (assistantAnimationFrameRef.current) {
        clearTimeout(assistantAnimationFrameRef.current as any);
        assistantAnimationFrameRef.current = null;
      }
      
             setUserFrequencyData(null);
             setAssistantFrequencyData(null);
             setIsConnected(false);
             setIsCalling(false);
             isCallingRef.current = false;
      
      console.log('æ–­å¼€ Voice Agent è¿æ¥');
    } catch (err) {
      console.error('æ–­å¼€è¿æ¥å¤±è´¥:', err);
    }
  }, []);

  /**
   * å¼€å§‹é€šè¯
   */
  const startCall = useCallback(async () => {
    if (!isConnected) {
      await connect();
    }
    
    try {
      if (!sessionRef.current) {
        throw new Error('Voice Agent æœªè¿æ¥');
      }
      
      // è·å– ephemeral keyï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
      const realtimeToken = await configApi.getRealtimeToken();
      
      // è·å– WebRTC ç«¯ç‚¹ URLï¼ˆä» transport æˆ– session ä¸­è·å–ï¼‰
      // æ³¨æ„ï¼šä¸è¦ä¼ é€’ url å‚æ•°ï¼Œè®© transport ä½¿ç”¨å®ƒè‡ªå·±çš„ baseUrl
      // å¦‚æœä¼ é€’äº† urlï¼Œä¼šè¦†ç›– transport çš„ baseUrlï¼Œå¯¼è‡´è·¯å¾„ä¸æ­£ç¡®
      const webrtcEndpoint = (sessionRef.current as any).__webrtcEndpoint;
      
      // æ£€æŸ¥ transport çš„å†…éƒ¨çŠ¶æ€
      const currentTransport = sessionRef.current?.transport;
      let transportInternalUrl = 'N/A';
      if (currentTransport instanceof OpenAIRealtimeWebRTC) {
        // å°è¯•è·å– transport çš„å†…éƒ¨ URLï¼ˆé€šè¿‡åå°„æˆ–ç›´æ¥è®¿é—®ï¼‰
        try {
          // @ts-ignore - è®¿é—®ç§æœ‰å±æ€§
          transportInternalUrl = currentTransport['#url'] || 'æ— æ³•è®¿é—®';
        } catch (e) {
          transportInternalUrl = 'æ— æ³•è®¿é—®ç§æœ‰å±æ€§';
        }
      }
      
      try {
        await sessionRef.current.connect({
          apiKey: realtimeToken.token,
          model: realtimeToken.model,
        });
      } catch (connectErr: any) {
        console.error('RealtimeSession è¿æ¥å¤±è´¥:', {
          error: connectErr,
          message: connectErr?.message,
          stack: connectErr?.stack,
          webrtcEndpoint: webrtcEndpoint,
          errorName: connectErr?.name,
          errorCause: connectErr?.cause,
        });
        
        // æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
        if (connectErr?.message?.includes('Failed to fetch')) {
          // æ£€æŸ¥æ˜¯å¦æ˜¯ CORS é—®é¢˜
          const isCorsError = connectErr?.message?.includes('CORS') || 
                             connectErr?.stack?.includes('CORS') ||
                             connectErr?.cause?.message?.includes('CORS');
          
          const errorMsg = `WebRTC è¿æ¥å¤±è´¥ (Failed to fetch)ã€‚

å¯èƒ½çš„åŸå› ï¼š
1. CORS é…ç½®é—®é¢˜ - æœåŠ¡å™¨æœªè®¾ç½®æ­£ç¡®çš„ CORS å¤´éƒ¨
2. æœåŠ¡å™¨ä¸æ”¯æŒ /v1/realtime/calls ç«¯ç‚¹
3. ç½‘ç»œè¿æ¥é—®é¢˜

è°ƒè¯•ä¿¡æ¯ï¼š
- WebRTC ç«¯ç‚¹: ${webrtcEndpoint}
- Transport å†…éƒ¨ URL: ${transportInternalUrl}
- æ˜¯å¦ CORS é”™è¯¯: ${isCorsError ? 'æ˜¯' : 'å¦'}

è¯·æ£€æŸ¥ï¼š
1. æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„ Network æ ‡ç­¾é¡µï¼ŒæŸ¥çœ‹å®é™…è¯·æ±‚çš„ URL å’Œå“åº”
2. æœåŠ¡å™¨æ˜¯å¦æ­£ç¡®é…ç½®äº† CORS å¤´éƒ¨ï¼ˆAccess-Control-Allow-Origin ç­‰ï¼‰
3. æœåŠ¡å™¨æ˜¯å¦æ”¯æŒ /v1/realtime/calls ç«¯ç‚¹`;
          setError(errorMsg);
          
          console.error('è¯¦ç»†é”™è¯¯ä¿¡æ¯:', {
            error: connectErr,
            webrtcEndpoint,
            transportInternalUrl,
            isCorsError,
            suggestion: 'è¯·æ‰“å¼€æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„ Network æ ‡ç­¾é¡µï¼ŒæŸ¥çœ‹å®é™…è¯·æ±‚çš„è¯¦ç»†ä¿¡æ¯',
          });
        }
        
        throw connectErr;
      }
      
      // ç­‰å¾…è¿æ¥å»ºç«‹åå†åˆå§‹åŒ–éŸ³é¢‘å¯è§†åŒ–
      const sessionTransport = sessionRef.current.transport;
      if (sessionTransport instanceof OpenAIRealtimeWebRTC) {
        await new Promise<void>((resolve) => {
          const checkConnection = () => {
            if (sessionTransport.status === 'connected') {
              resolve();
            } else {
              setTimeout(checkConnection, 100);
            }
          };
          checkConnection();
        });
        
        // ä» transport è·å–å®é™…çš„éŸ³é¢‘æµ
        // OpenAIRealtimeWebRTC å†…éƒ¨ä¼šè®¾ç½® audioElement.srcObject
        // æˆ‘ä»¬éœ€è¦ç­‰å¾…è¿™ä¸ªè®¾ç½®å®Œæˆ
        await new Promise<void>((resolve) => {
          const checkAudioElement = () => {
            if (assistantAudioElementRef.current?.srcObject) {
              resolve();
            } else {
              setTimeout(checkAudioElement, 100);
            }
          };
          setTimeout(() => resolve(), 5000);
          checkAudioElement();
        });
      }
      
      // å…ˆè®¾ç½® isCalling çŠ¶æ€ï¼Œè¿™æ ·éŸ³é¢‘å¯è§†åŒ–æ‰èƒ½æ­£å¸¸å·¥ä½œ
      setIsCalling(true);
      isCallingRef.current = true;
      
      // åˆå§‹åŒ–éŸ³é¢‘å¯è§†åŒ–
      if (userMediaStreamRef.current) {
        await initUserAudioVisualization(userMediaStreamRef.current);
      }
      
      if (assistantAudioElementRef.current) {
        const initAssistantVisualization = () => {
          if (assistantAudioElementRef.current?.srcObject) {
        initAssistantAudioVisualization(assistantAudioElementRef.current);
          } else {
            setTimeout(initAssistantVisualization, 200);
          }
        };
        setTimeout(initAssistantVisualization, 300);
      }
      
      console.log('å¼€å§‹è¯­éŸ³é€šè¯');
    } catch (err: any) {
      console.error('å¼€å§‹é€šè¯å¤±è´¥:', err);
      setError(err.message || 'å¼€å§‹é€šè¯å¤±è´¥');
      throw err;
    }
  }, [isConnected, connect, loadConfig, initUserAudioVisualization, initAssistantAudioVisualization]);

  /**
   * ç»“æŸé€šè¯
   */
  const endCall = useCallback(async () => {
    try {
      if (sessionRef.current) {
        sessionRef.current.close();
        sessionRef.current = null;
      }
      
      // åœæ­¢éŸ³é¢‘å¯è§†åŒ–
      if (userAnimationFrameRef.current) {
        cancelAnimationFrame(userAnimationFrameRef.current);
        userAnimationFrameRef.current = null;
      }
      if (assistantAnimationFrameRef.current) {
        cancelAnimationFrame(assistantAnimationFrameRef.current);
        assistantAnimationFrameRef.current = null;
      }
      
      // æ¸…ç†åŠ©æ‰‹éŸ³é¢‘æºå’Œä¸Šä¸‹æ–‡
      if (assistantSourceRef.current) {
        try {
          assistantSourceRef.current.disconnect();
        } catch (e) {
          // å¿½ç•¥æ–­å¼€è¿æ¥é”™è¯¯
        }
        assistantSourceRef.current = null;
      }
      
      if (assistantAudioContextRef.current) {
        try {
          assistantAudioContextRef.current.close();
        } catch (e) {
          // å¿½ç•¥å…³é—­é”™è¯¯
        }
        assistantAudioContextRef.current = null;
      }
      
      // æ¸…ç†éŸ³é¢‘æµ
      if (userMediaStreamRef.current) {
        userMediaStreamRef.current.getTracks().forEach((track) => track.stop());
        userMediaStreamRef.current = null;
      }
      
      setIsCalling(false);
      isCallingRef.current = false;
      setUserFrequencyData(null);
      setAssistantFrequencyData(null);
      
      console.log('ç»“æŸè¯­éŸ³é€šè¯');
    } catch (err: any) {
      console.error('ç»“æŸé€šè¯å¤±è´¥:', err);
      setError(err.message || 'ç»“æŸé€šè¯å¤±è´¥');
    }
  }, []);

  // æ¸…ç†
  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return {
    isConnected,
    isCalling,
    error,
    userFrequencyData,
    assistantFrequencyData,
    connect,
    disconnect,
    startCall,
    endCall,
  };
};
